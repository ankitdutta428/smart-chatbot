{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":165740,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141018,"modelId":163622}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing the Finetuned Phi-3.5-mini-instruct model for Customer Support Chatbot\n\n## Link for the code of the Finetuning of the Model\n\nhttps://www.kaggle.com/code/ankitd7752/finetuning-phi3-5-for-multiability-chatbot\n\nThe above is the link to the main code of the Customer Support Chatbot we have made for Working simultaneously in the domains of Healthcare, Telecommunication, Ecommerce, and Banking. The code is below is commented and is of the notebook above for reference.abs","metadata":{}},{"cell_type":"code","source":"# !pip install -q -U transformers\n# !pip install -q -U accelarate\n# !pip install -q -U bitsandbytes\n# !pip install -q -U trl\n# !pip install -q -U peft\n\n# model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n# dataset_path = \"/kaggle/input/training-data-for-multiability-chabot/final_training_data.csv\"\n\n# import os\n# import warnings\n# import pandas as pd \n# import numpy as np \n# import torch \n# import torch.nn as nn\n# from datasets import Dataset\n# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n# from peft import LoraConfig, PeftConfig\n# from trl import SFTTrainer, setup_chat_format\n\n# data = pd.read_csv('/kaggle/input/training-data-for-multiability-chabot/final_training_data.csv')\n# data.head(10)\n# data['category'].value_counts()\n\n# label_mapping = {\n#     'HC': 'Medical',\n#     'TC': 'Telecom',\n#     'BA': 'Banking',\n#     'EC': 'Ecommerce'\n# }\n\n# # Replace the labels\n# data['category'] = data['category'].replace(label_mapping)\n# print(data)\n# data = Dataset.from_pandas(data)\n# print(data)\n\n# columns_to_remove = [\"Unnamed: 0\", \"category_id\"]  # Replace with the names of the columns you want to drop\n# data = data.map(lambda x: x, remove_columns=columns_to_remove)\n# print(data)\n# compute_dtype = getattr(torch, 'float16')\n\n# bnbconfig = BitsAndBytesConfig(\n#     load_in_4bit = True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=compute_dtype,\n#     bnb_4bit_use_double_quant=True,\n# )\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_name, \n#     quantization_config=bnbconfig, \n#     torch_dtype=compute_dtype, \n#     low_cpu_mem_usage=True,\n#     #trust_remote_code=True,\n# )\n\n# def format_chat_template(row):\n#     row_json = [{\"role\": \"user\", \"content\": row[\"prompt\"]},\n#                {\"role\": \"assistant\", \"content\": row['response']}]\n#     row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n#     return row\n\n# dataset = data.map(\n#     format_chat_template,\n#     num_proc=4,\n# )\n\n# training_arguments = TrainingArguments(\n#     output_dir=\"RANDOM_CHATBOT\",\n#     num_train_epochs=1,\n#     per_device_train_batch_size=1,\n#     gradient_accumulation_steps=8,\n#     optim=\"paged_adamw_32bit\",\n#     save_steps=0,\n#     logging_steps=15,\n#     learning_rate=5e-4,\n#     weight_decay=0.001,\n#     fp16=True,\n#     max_grad_norm=0.3,\n#     warmup_ratio=0.03,\n#     lr_scheduler_type=\"cosine\",\n#     report_to=\"none\",\n# )\n\n# from peft import get_peft_model, prepare_model_for_kbit_training, PeftModel\n# peft_config = LoraConfig(\n#     r=16,\n#     lora_alpha=32,\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n# )\n# model = get_peft_model(model, peft_config)\n# trainer = SFTTrainer(\n#     model = model, \n#     train_dataset = dataset, \n#     peft_config = peft_config, \n#     tokenizer = tokenizer, \n#     args = training_arguments,\n#     #dataset_text_field=\"generated_prompt\",\n# )\n\n# trainer.train()\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": \"I am have been charged for 40gb of data while i used 30gb data only. Please resolve this issue\"\n#     }\n# ]\n\n# prompt = tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)\n# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n# outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n# text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# print(text.split(\"assistant\")[-1])\n\n# model_dir = \"Phi3.5-Combined-ChatBot\"\n# trainer.model.save_pretrained(model_dir)\n# trainer.tokenizer.save_pretrained(model_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing the libraries and the model from huggingface","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:39:48.772547Z","iopub.execute_input":"2025-01-25T10:39:48.772837Z","iopub.status.idle":"2025-01-25T10:39:56.385426Z","shell.execute_reply.started":"2025-01-25T10:39:48.772814Z","shell.execute_reply":"2025-01-25T10:39:56.384525Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"model_dir = 'ankitruler567/Phi-3.5-mini-instruct-Combined-Model'\nmodel = AutoModelForCausalLM.from_pretrained(model_dir)\ntokenizer = LlamaTokenizer.from_pretrained(model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:39:56.386439Z","iopub.execute_input":"2025-01-25T10:39:56.386848Z","iopub.status.idle":"2025-01-25T10:43:19.233799Z","shell.execute_reply.started":"2025-01-25T10:39:56.386823Z","shell.execute_reply":"2025-01-25T10:43:19.232933Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bac824a21874a9bababf298e060cf83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2f2788cf78f488ea06951e7fac92e3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f26dcf3edd240998cdcf3e181dbf04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d3a481a39dd4bab89ce4a979784c82f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e17f027dfe42c5aa77aa0b85ab9d40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f776e148b6149b182e8c12e94f67b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c6b013c670474cb75bf7c3c1631ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a5025c2d8524448b883a6b1a33660c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9230ec086e064f41b5c390d2fa6931f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57dadd88bfd9472088dfef844df418b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5886e549cc45f8acd9e0c65ecf5359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d5dc9f8bc4f40609ef83acc210ffb18"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Testing our code","metadata":{}},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"How do I enable international roaming on my number? \\n\" # Enter your prompts \n    }\n]\n\n# Apply the chat template to the message\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\n# Tokenize and move inputs to GPU\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\n# Move the model to GPU\nmodel = model.to(\"cuda\")\n\n# Generate outputs\noutputs = model.generate(**inputs, max_length=350, num_return_sequences=1)\n\n# Decode the output\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract and print the response\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:43:19.235008Z","iopub.execute_input":"2025-01-25T10:43:19.235497Z","iopub.status.idle":"2025-01-25T10:43:33.068806Z","shell.execute_reply.started":"2025-01-25T10:43:19.235474Z","shell.execute_reply":"2025-01-25T10:43:33.067895Z"}},"outputs":[{"name":"stdout","text":"|>  To enable international roaming on your number, please follow these steps:\n\n1. Log in to your account on {{WEBSITE_URL}}.\n2. Navigate to the {{ROAMING_SECTION}} section.\n3. Select the option to enable international roaming.\n4. Follow the on-screen instructions to complete the process.\n\nIf you encounter any issues, please contact our customer support team for further assistance. <|end|>\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Integrating Langchain with it","metadata":{}},{"cell_type":"markdown","source":"### Installing the necessary libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install langchain\n!pip install langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:43:33.069975Z","iopub.execute_input":"2025-01-25T10:43:33.070290Z","iopub.status.idle":"2025-01-25T10:43:46.856349Z","shell.execute_reply.started":"2025-01-25T10:43:33.070256Z","shell.execute_reply":"2025-01-25T10:43:46.855129Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Using Langchain ConservationBufferMemory\n\n#### We are facing an OutOfMemory Error due to less available GPU. We failed to find some good way to optimise it, or make our model work with the free version but have failed to do so till now. Feel free to comment or let us know of some ideas to optimise it. Thanks ! ","metadata":{}},{"cell_type":"code","source":"from langchain.schema import SystemMessage, HumanMessage, AIMessage\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer, pipeline\n\n# Set up the Hugging Face pipeline with 'do_sample=True' for varied responses\nhf_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0,  # Use GPU (0), or -1 for CPU\n    max_new_tokens=100,  # Allow up to 100 new tokens to be generated\n    pad_token_id=tokenizer.eos_token_id,\n    temperature=0.7,  # Controlled creativity\n    do_sample=True,  # Enable sampling for varied responses\n    truncation=True  # Explicitly enable truncation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:43:46.857691Z","iopub.execute_input":"2025-01-25T10:43:46.858055Z","iopub.status.idle":"2025-01-25T10:43:48.156889Z","shell.execute_reply.started":"2025-01-25T10:43:46.858020Z","shell.execute_reply":"2025-01-25T10:43:48.156068Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=hf_pipeline)\nmemory = ConversationBufferMemory(return_messages=True) \n\nprompt = ChatPromptTemplate.from_messages([\n    SystemMessage(content=\"You are a helpful and concise assistant that responds directly and accurately to user questions.\"),\n    MessagesPlaceholder(variable_name=\"history\"),  # Keeps the conversation flow natural\n    HumanMessage(content=\"{input}\")  \n])\n\nconversation = LLMChain(\n    llm=llm,\n    prompt=prompt,\n    memory=memory\n)\n\n# Chatbot loop\nprint(\"Chatbot: Hi! How can I assist you today?\")\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n        print(\"Chatbot: Goodbye!\")\n        break\n    memory.chat_memory.add_message(HumanMessage(content=user_input))\n    response = conversation.predict(input=user_input).strip()  # Strip any unnecessary whitespace\n    memory.chat_memory.add_message(AIMessage(content=response))\n    print(f\"Chatbot: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:43:48.157623Z","iopub.execute_input":"2025-01-25T10:43:48.157860Z","iopub.status.idle":"2025-01-25T10:45:51.622994Z","shell.execute_reply.started":"2025-01-25T10:43:48.157840Z","shell.execute_reply":"2025-01-25T10:45:51.621963Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-9fb4fa11bdc3>:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n<ipython-input-6-9fb4fa11bdc3>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferMemory(return_messages=True)\n<ipython-input-6-9fb4fa11bdc3>:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  conversation = LLMChain(\n","output_type":"stream"},{"name":"stdout","text":"Chatbot: Hi! How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  How do I set up auto-pay for my monthly bills?\n"},{"name":"stdout","text":"Chatbot: System: You are a helpful and concise assistant that responds directly and accurately to user questions.\nHuman: How do I set up auto-pay for my monthly bills?\nHuman: {input} To set up auto-pay for your monthly bills, please follow these steps:\n\n1. Log in to your online banking account or mobile app.\n2. Navigate to the 'Billing' or 'Payments' section.\n3. Select the billing service provider you want to enable auto-pay for.\n4. Choose the bills that you want to enable auto-pay for.\n5. Select the 'Auto-Pay' or\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  quit\n"},{"name":"stdout","text":"Chatbot: Goodbye!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}