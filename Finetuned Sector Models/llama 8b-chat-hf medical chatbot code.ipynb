{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning Llama 3\nWe’ll fine-tune the Llama 3 8B-Chat model using the ruslanmv/ai-medical-chatbot dataset. The dataset contains 250k dialogues between a patient and a doctor.","metadata":{}},{"cell_type":"markdown","source":"/kaggle/input/llama-3/transformers/8b-chat-hf/1\n\nGPU P100 \n\nhuggingface_token and wandb to be active\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:57:18.664666Z","iopub.execute_input":"2025-01-08T15:57:18.664883Z","iopub.status.idle":"2025-01-08T15:57:19.015716Z","shell.execute_reply.started":"2025-01-08T15:57:18.664862Z","shell.execute_reply":"2025-01-08T15:57:19.014849Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Ignore the warnings","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:57:30.370701Z","iopub.execute_input":"2025-01-08T15:57:30.371029Z","iopub.status.idle":"2025-01-08T15:57:30.374707Z","shell.execute_reply.started":"2025-01-08T15:57:30.371001Z","shell.execute_reply":"2025-01-08T15:57:30.373729Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:57:30.724262Z","iopub.execute_input":"2025-01-08T15:57:30.724599Z","iopub.status.idle":"2025-01-08T15:58:23.317545Z","shell.execute_reply.started":"2025-01-08T15:57:30.724571Z","shell.execute_reply":"2025-01-08T15:58:23.316598Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"transformers: A library for state-of-the-art natural language processing.\n\ndatasets: A library for easily accessing and sharing datasets.\n\naccelerate: A library for optimizing and accelerating model training.\n\npeft: A library for parameter-efficient fine-tuning.\n\ntrl: A library for training language models with reinforcement learning.\n\nbitsandbytes: A library for 8-bit optimizers and quantization.\n\nwandb: A tool for experiment tracking and model management.\n","metadata":{}},{"cell_type":"code","source":"# !pip uninstall peft huggingface_hub\n# !pip install peft==0.11.0 huggingface_hub==0.23.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:58:49.843038Z","iopub.execute_input":"2025-01-08T15:58:49.843343Z","iopub.status.idle":"2025-01-08T15:58:49.847117Z","shell.execute_reply.started":"2025-01-08T15:58:49.843321Z","shell.execute_reply":"2025-01-08T15:58:49.846030Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pip show peft huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:50:44.772148Z","iopub.execute_input":"2025-01-08T15:50:44.772545Z","iopub.status.idle":"2025-01-08T15:50:48.329963Z","shell.execute_reply.started":"2025-01-08T15:50:44.772515Z","shell.execute_reply":"2025-01-08T15:50:48.328703Z"}},"outputs":[{"name":"stdout","text":"Name: peft\nVersion: 0.14.0\nSummary: Parameter-Efficient Fine-Tuning (PEFT)\nHome-page: https://github.com/huggingface/peft\nAuthor: The HuggingFace team\nAuthor-email: benjamin@huggingface.co\nLicense: Apache\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\nRequired-by: \n---\nName: huggingface-hub\nVersion: 0.27.1\nSummary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\nHome-page: https://github.com/huggingface/huggingface_hub\nAuthor: Hugging Face, Inc.\nAuthor-email: julien@huggingface.co\nLicense: Apache\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: filelock, fsspec, packaging, pyyaml, requests, tqdm, typing-extensions\nRequired-by: accelerate, datasets, peft, timm, tokenizers, torchtune, transformers\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:58:54.966225Z","iopub.execute_input":"2025-01-08T15:58:54.966661Z","iopub.status.idle":"2025-01-08T15:59:09.381252Z","shell.execute_reply.started":"2025-01-08T15:58:54.966633Z","shell.execute_reply":"2025-01-08T15:59:09.380314Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"This code snippet is primarily setting up the necessary imports for a machine learning task involving natural language processing (NLP) using the Hugging Face Transformers library, PEFT (Parameter-Efficient Fine-Tuning), and other related tools. Here's a breakdown:\n\n\nHugging Face Transformers Imports:\n\n\n\nAutoModelForCausalLM, AutoTokenizer: For loading pre-trained language models and tokenizers.\n\nBitsAndBytesConfig, HfArgumentParser, TrainingArguments: For configuring model training and parsing arguments.\n\npipeline, logging: For creating NLP pipelines and logging.\n\n\nPEFT Imports:\n\n\n\nLoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model: For applying parameter-efficient fine-tuning techniques to models.\n\n\nOther Imports:\n\n\n\nos, torch, wandb: Standard libraries for operating system interactions, PyTorch (deep learning), and Weights & Biases (experiment tracking).\n\ndatasets: For loading datasets.\n\ntrl: Specific tools for training language models, including SFTTrainer and setup_chat_format.\n\n\n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:18.190123Z","iopub.execute_input":"2025-01-08T15:59:18.190745Z","iopub.status.idle":"2025-01-08T15:59:32.753386Z","shell.execute_reply.started":"2025-01-08T15:59:18.190719Z","shell.execute_reply":"2025-01-08T15:59:32.752488Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnabarupeducation\u001b[0m (\u001b[33mnabarupeducation-iit-kharagpur\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250108_155925-fnw9jsx5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/fnw9jsx5' target=\"_blank\">golden-leaf-3</a></strong> to <a href='https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/fnw9jsx5' target=\"_blank\">https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/fnw9jsx5</a>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:32.754585Z","iopub.execute_input":"2025-01-08T15:59:32.754860Z","iopub.status.idle":"2025-01-08T15:59:32.759175Z","shell.execute_reply.started":"2025-01-08T15:59:32.754838Z","shell.execute_reply":"2025-01-08T15:59:32.758242Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:32.760409Z","iopub.execute_input":"2025-01-08T15:59:32.760651Z","iopub.status.idle":"2025-01-08T15:59:32.776541Z","shell.execute_reply.started":"2025-01-08T15:59:32.760602Z","shell.execute_reply":"2025-01-08T15:59:32.775905Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Loading the model and tokenizer\n\nIn this part, we’ll load the model from Kaggle. However, due to memory constraints, we’re unable to load the full model. Therefore, we’re loading the model using 4-bit precision.\n\nOur goal in this project is to reduce memory usage and speed up the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:32.777522Z","iopub.execute_input":"2025-01-08T15:59:32.777806Z","iopub.status.idle":"2025-01-08T16:01:17.269382Z","shell.execute_reply.started":"2025-01-08T15:59:32.777777Z","shell.execute_reply":"2025-01-08T16:01:17.268726Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c70d6d78cf241c08024b6b38660f757"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Load the tokenizer and then set up a model and tokenizer for conversational AI tasks. By default, it uses the chatml template from OpenAI, which will convert the input text into a chat-like format.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:01:38.374589Z","iopub.execute_input":"2025-01-08T16:01:38.374895Z","iopub.status.idle":"2025-01-08T16:01:39.346201Z","shell.execute_reply.started":"2025-01-08T16:01:38.374872Z","shell.execute_reply":"2025-01-08T16:01:39.344907Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-dd49cade07a7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_chat_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/models/utils.py\u001b[0m in \u001b[0;36msetup_chat_format\u001b[0;34m(model, tokenizer, format, resize_to_multiple_of)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# check if model already had a chat template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;34m\"Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None"],"ename":"ValueError","evalue":"Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## Adding the adapter to the layer\nFine-tuning the full model will take a lot of time, so to improve the training time, we’ll attach the adapter layer with a few parameters, making the entire process faster and more memory-efficient.\n\n\n","metadata":{}},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:01:46.418415Z","iopub.execute_input":"2025-01-08T16:01:46.418754Z","iopub.status.idle":"2025-01-08T16:01:47.176548Z","shell.execute_reply.started":"2025-01-08T16:01:46.418726Z","shell.execute_reply":"2025-01-08T16:01:47.175649Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Loading the dataset\nTo load and pre-process our dataset, we:\n\n1. Load the ruslanmv/ai-medical-chatbot dataset, shuffle it, and select only the top 1000 rows. This will significantly reduce the training time.\n\n2. Format the chat template to make it conversational. Combine the patient questions and doctor responses into a \"text\" column.\n\n3. Display a sample from the text column (the “text” column has a chat-like format with special tokens).\n\n\n","metadata":{}},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:01:51.528426Z","iopub.execute_input":"2025-01-08T16:01:51.528728Z","iopub.status.idle":"2025-01-08T16:02:00.765654Z","shell.execute_reply.started":"2025-01-08T16:01:51.528706Z","shell.execute_reply":"2025-01-08T16:02:00.764928Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846ac3e143604a15a685cae92115813e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dialogues.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb74c15d94e4a458313ff36611d8c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86eb16101814580afeb2247636eaf1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eb601abea7b47b584928ed186055ddb"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"4. Split the dataset into a training and validation set.\n","metadata":{}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:06.831654Z","iopub.execute_input":"2025-01-08T16:02:06.831997Z","iopub.status.idle":"2025-01-08T16:02:06.846572Z","shell.execute_reply.started":"2025-01-08T16:02:06.831971Z","shell.execute_reply":"2025-01-08T16:02:06.845626Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Complaining and training the model\nWe are setting the model hyperparameters so that we can run it on the Kaggle. You can learn about each hyperparameter by reading the Fine-Tuning Llama 2 tutorial.\n\nWe are fine-tuning the model for one epoch and logging the metrics using the Weights and Biases.\n\n\n","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:10.899676Z","iopub.execute_input":"2025-01-08T16:02:10.900036Z","iopub.status.idle":"2025-01-08T16:02:10.935399Z","shell.execute_reply.started":"2025-01-08T16:02:10.900008Z","shell.execute_reply":"2025-01-08T16:02:10.934530Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"This code snippet is configuring the training parameters for a machine learning model using the TrainingArguments class. Here's a breakdown of the most relevant parts:\n\n\n\noutput_dir=new_model: Specifies the directory where the trained model and other outputs will be saved.\n\nper_device_train_batch_size=1 and per_device_eval_batch_size=1: Sets the batch size for training and evaluation to 1 per device.\n\ngradient_accumulation_steps=2: Accumulates gradients over 2 steps before performing a backward pass, effectively simulating a larger batch size.\n\noptim=\"paged_adamw_32bit\": Chooses the optimizer, in this case, a 32-bit version of AdamW.\n\nnum_train_epochs=1: Sets the number of training epochs to 1.\n\nevaluation_strategy=\"steps\" and eval_steps=0.2: Specifies that evaluation should be done every 0.2 steps.\n\nlogging_steps=1 and logging_strategy=\"steps\": Logs training metrics every step.\n\nwarmup_steps=10: Sets the number of warmup steps for learning rate scheduling.\n\nlearning_rate=2e-4: Sets the learning rate to 0.0002.\n\nfp16=False and bf16=False: Disables 16-bit and bfloat16 precision training.\n\ngroup_by_length=True: Groups sequences of similar lengths together to optimize training efficiency.\n\nreport_to=\"wandb\": Specifies that training metrics should be reported to Weights and Biases (wandb) for tracking.\n","metadata":{}},{"cell_type":"code","source":"# Add a new pad_token to the tokenizer\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))  # Update model's embeddings to include the new token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:16.349463Z","iopub.execute_input":"2025-01-08T16:02:16.349796Z","iopub.status.idle":"2025-01-08T16:02:46.240545Z","shell.execute_reply.started":"2025-01-08T16:02:16.349767Z","shell.execute_reply":"2025-01-08T16:02:46.239863Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Embedding(128257, 4096)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Preprocessing function to tokenize and truncate/pad sequences\ndef preprocess_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n# Apply preprocessing to train and test datasets\ntokenized_train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\ntokenized_test_dataset = dataset[\"test\"].map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:49.350992Z","iopub.execute_input":"2025-01-08T16:02:49.351293Z","iopub.status.idle":"2025-01-08T16:02:50.488844Z","shell.execute_reply.started":"2025-01-08T16:02:49.351270Z","shell.execute_reply":"2025-01-08T16:02:50.487867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f2e32db52449859ef9bfc88f907f39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a2a28d08a24357b36ea8c74d151b14"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# We’ll now set up a supervised fine-tuning (SFT) trainer and provide\n# a train and evaluation dataset, LoRA configuration, training argument, \n# tokenizer, and model. We’re keeping the max_seq_length to 512 to avoid \n# exceeding GPU memory during training.\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:54.787139Z","iopub.execute_input":"2025-01-08T16:02:54.787451Z","iopub.status.idle":"2025-01-08T16:02:55.015103Z","shell.execute_reply.started":"2025-01-08T16:02:54.787427Z","shell.execute_reply":"2025-01-08T16:02:55.014228Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:56.879510Z","iopub.execute_input":"2025-01-08T16:02:56.879841Z","iopub.status.idle":"2025-01-08T16:36:07.518264Z","shell.execute_reply.started":"2025-01-08T16:02:56.879812Z","shell.execute_reply":"2025-01-08T16:36:07.517538Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 33:05, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>1.975800</td>\n      <td>2.512768</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.524000</td>\n      <td>2.487967</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.125000</td>\n      <td>2.454458</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.673300</td>\n      <td>2.422145</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.527800</td>\n      <td>2.415500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=2.463011441230774, metrics={'train_runtime': 1989.7227, 'train_samples_per_second': 0.452, 'train_steps_per_second': 0.226, 'total_flos': 2.08655911747584e+16, 'train_loss': 2.463011441230774, 'epoch': 1.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Model evaluation\nWhen you finish the Weights & Biases session, it’ll generate the run history and summary.\n","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:39:57.884614Z","iopub.execute_input":"2025-01-08T16:39:57.884931Z","iopub.status.idle":"2025-01-08T16:40:00.792586Z","shell.execute_reply.started":"2025-01-08T16:39:57.884909Z","shell.execute_reply":"2025-01-08T16:40:00.791991Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▄▁▁</td></tr><tr><td>eval/runtime</td><td>▂▁▂█▂</td></tr><tr><td>eval/samples_per_second</td><td>███▁▇</td></tr><tr><td>eval/steps_per_second</td><td>███▁▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▅▄▄▅▄▃▆▃▃▂▃▅▅▃▄▃▄▃▃▃▄▃▁▃▃▄▃▅▃▃▃▆▅▄▃▂▄▃▃</td></tr><tr><td>train/learning_rate</td><td>▄▅███▇▇▇▇▇▆▅▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▇▅▄▃▄▅▂▁▄▇▆▂▃▅▄▃▂█▆▄▃▅▇▄▁▄▃█▄▇▃▃▆▃▂▃▁▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.4155</td></tr><tr><td>eval/runtime</td><td>83.9591</td></tr><tr><td>eval/samples_per_second</td><td>1.191</td></tr><tr><td>eval/steps_per_second</td><td>1.191</td></tr><tr><td>total_flos</td><td>2.08655911747584e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>1.53013</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>2.5278</td></tr><tr><td>train_loss</td><td>2.46301</td></tr><tr><td>train_runtime</td><td>1989.7227</td></tr><tr><td>train_samples_per_second</td><td>0.452</td></tr><tr><td>train_steps_per_second</td><td>0.226</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">golden-leaf-3</strong> at: <a href='https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/fnw9jsx5' target=\"_blank\">https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/fnw9jsx5</a><br> View project at: <a href='https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/nabarupeducation-iit-kharagpur/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250108_155925-fnw9jsx5/logs</code>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have a bad scar on my forehead. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:40:43.457154Z","iopub.execute_input":"2025-01-08T16:40:43.457474Z","iopub.status.idle":"2025-01-08T16:40:59.182837Z","shell.execute_reply.started":"2025-01-08T16:40:43.457451Z","shell.execute_reply":"2025-01-08T16:40:59.182030Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nHi. For the scar on your forehead, you can use silicone gel sheeting. It is available in the market. You can apply it on the scar and leave it overnight. Repeat this process for a few days. You can also use vitamin E oil on the scar. Apply it on the scar and leave it overnight. Repeat this process for a few days. You can also use aloe vera gel on the scar. Apply it on the scar and leave it overnight. Repeat this process for a few days. Hope I have answered your query. Let me know if I can assist you further.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Saving the model file\nWe’ll now save the fine-tuned adapter and push it to the Hugging Face Hub. The Hub API will automatically create the repository and store the adapter file.\n\n\n","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:41:12.807564Z","iopub.execute_input":"2025-01-08T16:41:12.808021Z","iopub.status.idle":"2025-01-08T16:43:05.333833Z","shell.execute_reply.started":"2025-01-08T16:41:12.807983Z","shell.execute_reply":"2025-01-08T16:43:05.333056Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9b88629a1e40718f7bff01f2667ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038e729889ee42b98d9f4a35407773d8"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/NabarupGhosh/llama-3-8b-chat-doctor/commit/385d95eed51dc630a0bfaa3809c51e57d4a6d8a0', commit_message='Upload model', commit_description='', oid='385d95eed51dc630a0bfaa3809c51e57d4a6d8a0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/NabarupGhosh/llama-3-8b-chat-doctor', endpoint='https://huggingface.co', repo_type='model', repo_id='NabarupGhosh/llama-3-8b-chat-doctor'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}